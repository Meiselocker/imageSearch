{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imageSearch\n",
    "\n",
    "## A Python console application for biometric identification, specifically designed for wildlife biologists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "[About this application](#About-this-application)\n",
    "\n",
    "* [Motivation](#Motivation)\n",
    "* [What you can do with this application](#What-you-can-do-with-this-application)\n",
    "* [A brief presentation of the algorithm](#A-brief-presentation-of-the-algorithm)\n",
    "* [The importance of image preprocessing](#The-importance-of-image-preprocessing)\n",
    "\n",
    "[Tutorial](#Tutorial)\n",
    "* [Data for this tutorial](#Data-for-this-tutorial)\n",
    "* [Instantiating and loading a DB object](#Instantiating-and-loading-a-DB-object)\n",
    "* [Reading your data into the register](#Reading-your-data-into-the-register)\n",
    "* [Managing your register](#Managing-your-register)\n",
    "* * [Deleting entries](#Deleting)\n",
    "* * [Assigning an ID to an image](#Assigning-an-ID-to-an-image)\n",
    "* * [Exploring your data](#Exploring-your-data)\n",
    "* [Verifying identities and matching images](#Verifying-identities-and-matching-image)\n",
    "* [Saving and reloading your database](#Saving-and-reloading-your-DB) \n",
    "* [Training and testing your model](#Training-and-testing-your-model)\n",
    "\n",
    "[A few suggestions for optimal use](#A-few-suggestions-for-optimal-use)\n",
    "* [Use the help function](#Use-the-help-function)\n",
    "* [Similarity score and ranking](#Similarity-score-and-ranking)\n",
    "* [Picking the right threshold](#Picking-the-right-threshold)\n",
    "* [Looking at the ranking ratio](#Looking-at-the-ranking-ratio)\n",
    "* [Performance and computing time: parameter tuning](#Performance-and-computing-time:-parameter-tuning)\n",
    "* * [Resolution](#Resolution)\n",
    "* * [Blurring](#Blurring)\n",
    "* * [Other parameters for advanced users](#Other-parameters-for-advanced-users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation \n",
    "\n",
    "Wildlife biologists that study a certain species often capture individuals and release them later. When capturing individuals on the site, it is paramount to determine, whether the captured individuals have *already been captured* previously, and they have, what their identity is.\n",
    "\n",
    "To be able to identify individuals in case of recapture, biologists often tag them before releasing them. However, tagging can be problematic in some cases: they can impair individuals' survival in the wild, and they can also be damaged or lost. A possible solution to this problem is image biometric identification. \n",
    "\n",
    "This challenge is faced by many other applications involving human faces. For example, when trying to enter a secured facility, access will be granted if a photo, taken on the spot, can match with one of the indexed images of the staff members (one-to-many matching). \n",
    "\n",
    "Using neural networks, very performant, fast models have been developed for face identification. Thanks to massive amounts of data, these models have learned to extract light, highly distinguishable representations from face images. In wildlife research, however, data is often unsifficient to train such models. In addition, there is a need for a versatile model, able to accommodate a variety of different-looking species.\n",
    "\n",
    "This console Python application proposes biometric identification based on keypoints detection and feature matching. Its model can be trained with small amounts of data, and preliminary testing has demonstrated its ability to identify accurately species as different as toads and humans.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What you can do with this application\n",
    "\n",
    "* Manage a register of images for your population.\n",
    "* Authentify and identify new images against the indexed images of your register. \n",
    "* Train a model to accommodate your species and your type of images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A brief presentation of the algorithm\n",
    "\n",
    "The proposed program is based on [detection and description of keypoints (or features) in images](https://en.wikipedia.org/wiki/Feature_detection_(computer_vision)), a well-studied topic, for which many implementations exist. Here we used the (patented) SURF algorithm, implemented in openCV. There are several alternative to the SURF algorithm, such as ORB or FAST, which can yield good results with a little adjustment. In short, keypoints are areas in an the image where pixel intensities differ importantly and for which descriptors containing information about neighboring pixels, scale, orientation etc are computed ([details on the specific openCV implementation can be found here)](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_surf_intro/py_surf_intro.html). For more details, a good overview of image descriptors in openCV is to be found in the chapter 6  of J.Howe's and J. Minichino's [Learning opvenCV 4 computer vision with Python 3](https://www.amazon.com/Learning-OpenCV-Computer-Vision-Python-ebook/dp/B084ZH43LV).\n",
    "\n",
    "*Here is an example of a keypoints (i.e. blobs of contrasting pixels) found using SURF:* \n",
    "<img src=\"files/surf_ex.jpg\" alt=\"Drawing\" style=\"width: 400px;\">\n",
    "\n",
    "Descriptors of keypoints across any pair of images A and B can be therefore compared and the distance between the arrays of descriptors (here, the **norm** of the difference between the descriptor arrays; not to be confused between the **Euclidian distance** between the locations of the keypoints) can be calculated. Among the best matches, we filter false matches with the Lowe's ratio test, which is a good indicator for match quality: the distance score for the best match between keypoints have to smaller than a threshold fraction (say 0.8) of the distance scores for the second-best match. \n",
    "\n",
    "*Here is an example of matching keypoins (colored circles) between two different images of the same toad (left and right):*\n",
    "<img src=\"files/matches.jpg\" alt=\"Drawing\" style=\"width: 600px;\">\n",
    "\n",
    "The algorithm further filters one-to-many matches between keypoints, as these are typically spurious. \n",
    "\n",
    "Using the best matches, we find out the best [homography transformation](https://en.wikipedia.org/wiki/Homography), i.e. the reprojection of the image A according to the camera view of image B. With the RANSAC algorithm for outlier detection, we can identify matches that are not consistent with the image reprojection. This does not only allow for a better selection of matches, but also to assess the overall similarity between the images.\n",
    "\n",
    "\n",
    "*In the left-hand column of the figure below, we see image A before and after being warped by the homography matrix (top and bottom); the right-hand column represents image B, the destination. We can see that image A is warped so as to fit the perspective of image B around the matched keypoints.*\n",
    "<img src=\"files/homography.jpg\" alt=\"Drawing\" style=\"width: 400;\">\n",
    "\n",
    "In addition to the distance between the descriptors (i.e. how dissimilar they are), we further measure the Euclidian distance between the *locations* of the best-matching keypoints. Combined with the homography data, these help get a good idea of the overall coherence of the matches between the images.\n",
    "\n",
    "Thus, for a given set of a varying number of the matches in a pair of images, we obtain (a) the number of good matches, (b) the distances between matched descriptors, of which we take the mean and the standard deviation (c) the coeficients of the Homography matrix. We feed these data into a stacking model consisting of a [*support vector machine*] (https://en.wikipedia.org/wiki/Support_vector_machine) with radial basis function kernel and a [*random forest classifier*](https://en.wikipedia.org/wiki/Random_forest), assigning a probability of similarity between pairs of images. Thus, the model can output a similarity score between two images for authentification (one-to-one matching), determine whether an individual exists in the register, and if it does, which it is (one-to-many matching)\n",
    "\n",
    "### The importance of image preprocessing\n",
    "The efficiency of the algorithm described above in matching between images of the same individual depends strongly on the preprocessing of the images. \n",
    "\n",
    "First, image denoising can speed up the process of matching, as small noise areas of the image can be detected as keypoints. Here we apply a Guassian blur to denoise the image. \n",
    "\n",
    "Second, the contrast is equalized locally using the [CLAHE technique](https://en.wikipedia.org/wiki/Adaptive_histogram_equalization#Contrast_Limited_AHE), as photos are taken under varying luminance conditions. Indeed, we need to equalize brigtness and contrast, as matching can be impaired by regions of the photographed individual that are darker in one image than in another. \n",
    "\n",
    "Last, images are converted to gray scale to ensure a better and fast processing. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data for this tutorial\n",
    "\n",
    "This tutorial shows many examples from real data. The application comes with a dataset of green toads *Bufotes viridis* (see README for download line). Let's download it into the project folder (i.e. where the scripts are), name the folder `data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating and loading a DB object\n",
    "First, we import the `database` module, as well as the `utils` module that comes with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import database\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instantiate a new `DB` object, we need to passe the size of the images that will be read into the register. E.g. for a 500x600:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = database.DB(target_size = (500,600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can load a DB object created previously with the `load_DB(path)` function from the `utils` module: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading your data into the register"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your `DB` object exists, you can read your data into the register. Let's examine four cases: \n",
    "1. You have **multiple images** of captured individuals, to which *an ID can be assigned with certainty*. This can be the case when you go to a new site, and all captured individuals are certainly new. Thus, you want to assign each of them a new ID. \n",
    "\n",
    "    The application supports reading of multiple image files organized in subfolders, where each sub-older corresponds to an individual. By calling `DB.add_entries_from_folder(path)`, the application infers the ID that you want to assign to an individual from the subfolder's name. \n",
    "\n",
    "    E.g., we have two individuals, Alice and Bob, with one or multiple images for each. We organize our data in a folder `data` with subfolders `Bob` and `Alice`:\n",
    "    ```\n",
    "    data\n",
    "    |-- Bob\n",
    "        |-- img_of_bob.jpg\n",
    "    |-- Alice\n",
    "        |-- img_of_alice_0.jpg\n",
    "        |-- img_of_alice_1.jpg`\n",
    "\n",
    "    ```\n",
    "    \n",
    "    The dataset of green toads (see README for download line) is organized in subfolders as required. We will use it to illustrate our point.   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Registering images...\n",
      "[####################] 100% \n",
      " \n"
     ]
    }
   ],
   "source": [
    "foo.add_entries_from_folder(\"data/toads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once new data have now been registered and each image has been assigned a unique key and the identity of the individual that it represents. \n",
    "\n",
    "We can view the register (or parts of it) with the method `DB.view_register()`. By passing a list of keys, we can specify the keys that we want to view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*Key*   *Assigned_id*   *Images_for_id*   *Path*                           \n",
      "1       001             [0, 1, 2]         data/toads/001/Ind_001_002.JPG   \n",
      "13      007             [13, 14]          data/toads/007/Ind_007_001.JPG   "
     ]
    }
   ],
   "source": [
    "foo.view_register([1,13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. You want to register **multiple images**, whose **ID is unknown** (you don't know whether they have been captured before). You can use the same method, but specify that ID should not be inferred from subfolders' names:  `DB.add_entries_from_folder(path, id_from_folder = False)`. Entries will be assigned the a NA value as ID.\n",
    "\n",
    "\n",
    "3. You want to register a **single image**, **whose ID you know**:  `DB.add_entry(path, identity = 'my_id')`.\n",
    "\n",
    "\n",
    "4. You want to register a **single image**, of which **you don't know the ID**: `DB.add_entry(path)`. The image will be assigned a NA value as ID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing your register"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The application offers basic functionalities for managing your register.\n",
    "#### Deleting\n",
    "You can delete a specific entry by calling `DB.delete_entry(entry_key)`. If you want to delete all images associated with an ID, call `DB.delete_id(id)`.\n",
    "#### Assigning an ID to an image\n",
    "If you want to (re-) assign an ID to a certain image, call `DB.assign_id(entry_key, id)`. Pass as arguments the key of the image you want to assign an ID to and the ID (string).\n",
    "#### Exploring your data\n",
    "Apart from `DB.view_register()` introduced before, you can explore the data in your register using `DB.image_for_id(id)`. This method outputs all image keys associated with an id. Let's see what images are associated with ID `011` and `test`. The latter are images of various individuals which will be used for testing later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'011': [22, 23, 24], 'test': [141, 142, 143, 144, 145, 146, 147]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo.image_for_id(['011', 'test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying identities and matching images\n",
    "\n",
    "The core functionality of the application is image verification. For a new image whose ID is unknown, we can check whether similar images exist, thus suggesting that the individual probably exists in the register. \n",
    "Let's illustrate this functionality by matching images labelled as \"test\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': [141, 142, 143, 144, 145, 146, 147]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo.image_for_id('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Photos 141-147 are test images, which belong to different individuals. Each image has two 'siblings' labelled with their true identitiy. The program does not know what which they are, only the file name tells us the true identity of the test images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*Key*   *Assigned_id*   *Images_for_id*                       *Path*                            \n",
      "141     test            [141, 142, 143, 144, 145, 146, 147]   data/toads/test/Ind_001_004.JPG   \n",
      "142     test            [141, 142, 143, 144, 145, 146, 147]   data/toads/test/Ind_011_004.JPG   \n",
      "143     test            [141, 142, 143, 144, 145, 146, 147]   data/toads/test/Ind_016_004.JPG   \n",
      "144     test            [141, 142, 143, 144, 145, 146, 147]   data/toads/test/Ind_024_004.JPG   \n",
      "145     test            [141, 142, 143, 144, 145, 146, 147]   data/toads/test/Ind_030_004.JPG   \n",
      "146     test            [141, 142, 143, 144, 145, 146, 147]   data/toads/test/Ind_042_004.JPG   \n",
      "147     test            [141, 142, 143, 144, 145, 146, 147]   data/toads/test/Ind_058_004.JPG   "
     ]
    }
   ],
   "source": [
    "foo.view_register(foo.image_for_id('test')['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take, for instance, image # 141. We call `DB.verify_with_all(key, n)` to match pairwise between the image registered under `key` and for displaying the `n` best matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9819, '001', 2),\n",
       " (0.9777, '001', 0),\n",
       " (0.9607, '001', 1),\n",
       " (0.0016, '029', 67),\n",
       " (0.0014, '002', 3),\n",
       " (0.0014, '002', 4)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo.verify_with_all(141, n = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the model correctly detected images 2,0,1, belonging to individual '001' as being most resemblant to image # 141, with match level > 90%, against < 1% for other images. We can safely reassign '001' as ID to image # 141:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "picture 141 was assigned the id 001\n"
     ]
    }
   ],
   "source": [
    "foo.assign_id(141, '001')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reassign the IDs for the rest of the test images following the same procedure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of doubt, we can inspect the match between two images with `DB.match(pair, show = True, force_rematch = True)`.  We pass (147,140) as `pair`, and we choose to `show` the matching images, and to `force_rematch`, as the matching operation was already carried out once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo.match((147,140), show = True, force_rematch = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An 'manual' inspection would have allowed us to determine with certainty that images # 140 and # 147 are siblings, even the modelwas uncertain about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and reloading your DB\n",
    "The DB class has a method `DB.save(path)` for saving the database. Reloading the database can be done with the function `load_DB(path)` of the `utils` module. Since opencv keypoints can't be saved, these functions convert them to Python lists of arrays when saving and back to opencv keypoints when loading. Therefore, loading can take some time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading database...\n",
      "creating keypoints...\n",
      "[####################] 100% \n"
     ]
    }
   ],
   "source": [
    "foo.save(\"foo.pkl\")\n",
    "foo = utils.load_DB(\"foo.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing your model\n",
    "\n",
    "To output a similarity score between images, the program relies on a feature-based machine learning classification model. Optimal decision boundaries might differ across problem types. The default model (\"model.pkl\") was trained to optimally verify *toad images*. Other types of images might require a new training of the model. To show how you can do it, let's consider a toy example with human faces (there are, of course, many other outstanding, performant and much more effective models for face verification). The Utrecht face dataset [http://pics.stir.ac.uk/2D_face_sets.htm] is a simple dataset consisting (mainly) of pairs of images with the same person, smiling in one, and with neutral face in the other. We have preprepared the data, so that it can fit the folder organization required by the program. The link to the data is in the README file. \n",
    "\n",
    "We call a new DB instance and read the images in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Registering images...\n",
      "[####################] 100% \n",
      " \n"
     ]
    }
   ],
   "source": [
    "test = database.DB((150,200))\n",
    "test.add_entries_from_folder(\"data/utrecht\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide that we split our data into two equal parts for training and testing. \n",
    "\n",
    "Now we can call `DB.train_model(train_prop, clf = None, save_model = None)`. We save our model as `face_model.pkl`. If you want this model to be your default model, save it as `model.pkl`. You can pass your favorite sklearn classifier as argument (`clf`), but by default, the stacking classifier mentioned in the presentation of the algorithm will be used.\n",
    "\n",
    "The training depends on the image size and, of course, the sample number, and can be very long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Matching pairs of images...\n",
      "\n",
      "[####################] 100% \n",
      "Confusion matrix for training dataset\n",
      "[[1736    0]\n",
      " [   0   34]]\n"
     ]
    }
   ],
   "source": [
    "my_model = test.train_model(train_prop = 0.5, save_model = \"utrecht_face_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to test the performance of such a model. Here we use the following method: each image is matched with one image from all labelled individual in the register. For each image, we rank the matched images by similarity score and search through the ranking for the/a twin image (i.e. image of the same individual). The higher the position of the twin image in the ranking, the better the model. \n",
    "\n",
    "The program assumes by default that the test individuals are those previously defined by `DB.train_model()`, as in this example. If you want to determine it yourself, or if you didn't train the model at all, you can choose to randomly assign individuals to the testing, by passing to `DB.test_model()` either a list of ID's or a float corresponding to the proportion to be used for testing. \n",
    "\n",
    "Here, we have pairs of images and the testing ID's were assigned by the training function so we just have to call `DB.test_model()`. The method returns a 2D array with three columns: keys, scores (i.e. ranking of the corresponding twin images), and (one of the) true twin(s). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[####################] 100% \n"
     ]
    }
   ],
   "source": [
    "res = test.test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view how good the model performed, we can check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of right matches in first places of ranking: 100.00%\n",
      "% of right matches in first 3 places of ranking: 100.00%\n",
      "% of right matches in first 5 places of ranking: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"% of right matches in first places of ranking: {:2.2%}\".format(np.mean(res[:,1]==0)))\n",
    "print(\"% of right matches in first 3 places of ranking: {:2.2%}\".format(np.mean(res[:,1]<=2)))\n",
    "print(\"% of right matches in first 5 places of ranking: {:2.2%}\".format(np.mean(res[:,1]<=4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our model is saved an to our satisfaction, we can load it at any time after creating a DB object with `DB.load_model(path)`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A few suggestions for optimal use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the help function\n",
    "\n",
    "Not all the features are covered in this tutorial. You can use the python `help` function on a method to access the full documentation. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method match in module database:\n",
      "\n",
      "match(key_tuple, show=False, force_rematch=False, verbose=True) method of database.DB instance\n",
      "    Matches descriptors of keypoints in two images, and creates open-cv match objects,\n",
      "    selected keypoints and a homography matrix, from which\n",
      "    match similarity features are extracted. They are subsequently used to assess\n",
      "    degree of identity between the images.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    key_tuple : tuple of two INT.\n",
      "        Keys of the image to be matched.\n",
      "    show : BOOL, optional\n",
      "        Whether to show the pair of images with the keypoints and drawn matches.\n",
      "        The default is False\n",
      "    force_rematch : BOOL, optional\n",
      "        If match already exsists whether to rematch. The default is False.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    None.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import database\n",
    "import utils\n",
    "example = database.DB((300,400))\n",
    "help(example.match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity score and ranking\n",
    "\n",
    "The testing method proposed here is based on ranking. Ranking is more robust to changes in image types, such as contrast or resolution, since the level of similarity is **relative**. However, when you don't know  *whether* a twin image actually exists in the register, you need to rely on the similarity score rather than on ranking, and set a decision threshold between above which one can consider a pair of images as belonging to the same individual. However, using a model trained on different types, such changes can impact the similarity score magnitude. \n",
    "\n",
    "**It is therefore preferrable to retrain your model whenever dealing with new types of data** (different resolution, contrast, picture frame, and of course species). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picking the right threshold\n",
    "\n",
    "If you can't retrain the model, be careful about the decision threshold. In an optimally trained model, the decision threshold should be at 50%. When importing a model trained on data with different characteristics, the model can still suit your needs, but it might output probabilities such that you need to set your decision threshold at a different level. For example, when all images have a lower resolution or lower contrast, the number of good matches between keypoints will be lower, and therefore a model trained on high resoultion images might predict lower similarity probabilities. It is therefore important to test the model with a few labelled data to adjust the decision threshold to the new type of images. \n",
    "\n",
    "To illustrate this point, let's try to use the default model, trained on 500x600 toad images, on the Utrecht face database, reducing images to a 300x400 resolution. We will take 10 pairs of data, for which the identity is known, to test out our model on this new type of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Registering images...\n",
      "[####################] 100% \n",
      " \n",
      "Twin keys of 20 are [19]\n",
      "[(0.0028, '19', 22, 1.037), (0.0027, '19', 21, 1.0), (0.0027, '2', 23, 1.0)]\n",
      "Twin keys of 22 are [21]\n",
      "[(0.41, '19', 21, 146.4286), (0.0028, '18', 20, 1.0), (0.0028, '2', 24, 1.0)]\n",
      "Twin keys of 24 are [23]\n",
      "[(0.0967, '2', 23, 34.5357), (0.0028, '19', 21, 1.0), (0.0028, '19', 22, 1.0)]\n",
      "Twin keys of 26 are [25]\n",
      "[(0.3034, '20', 25, 108.3571), (0.0028, '19', 21, 1.0), (0.0028, '19', 22, 1.037)]\n",
      "Twin keys of 28 are [27]\n",
      "[(0.1893, '21', 27, 67.6071), (0.0028, '19', 21, 1.0), (0.0028, '19', 22, 1.0)]\n",
      "Twin keys of 30 are [29]\n",
      "[(0.0876, '22', 29, 31.2857), (0.0028, '19', 22, 1.037), (0.0027, '18', 20, 1.0)]\n",
      "Twin keys of 32 are [31]\n",
      "[(0.1445, '23', 31, 51.6071), (0.0028, '24', 33, 1.037), (0.0027, '18', 20, 1.0)]\n",
      "Twin keys of 34 are [33]\n",
      "[(0.1185, '24', 33, 42.3214), (0.0028, '19', 22, 1.037), (0.0027, '18', 20, 1.0)]\n",
      "Twin keys of 36 are [35, 37]\n",
      "[(0.6505, '25', 35, 34.418), (0.0189, '25', 37, 6.75), (0.0028, '19', 22, 1.037)]\n",
      "Twin keys of 38 are [39]\n",
      "[(0.1188, '26', 39, 44.0), (0.0027, '18', 20, 1.0), (0.0027, '19', 21, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import database\n",
    "import utils\n",
    "example = database.DB((300,400))\n",
    "example.load_model(\"utrecht_face_model.pkl\")\n",
    "example.add_entries_from_folder(\"data/utrecht\")\n",
    "keys_subsample = np.arange(20,40)\n",
    "#we test every other image, one per pair\n",
    "for key in keys_subsample[::2]:\n",
    "    print(\"Twin keys of {} are {}\".format(key, example.view_twin_keys(key)))\n",
    "    print(example.verify_with_batch(key,keys_subsample, return_ratio = True)[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the model is still rather accurate (the true twin image appears mostly at the first place), although it was trained with a different resolution. Setting the threshold at 1% or 2% will still yield good results, but it would be very dangerous to set the threshold at 50%.  \n",
    "\n",
    "\n",
    "### Looking at the ranking ratio\n",
    "\n",
    "The ranking ratio is the ratio between the similarity score ranked i and i+1. A low ranking ratio (close to one) should rings an alarm bell since two images have been assigned a close similarity score. It can mean, of course, that the image has several twin images in the register, but also that the model performs poorly and that the image fed in fails to produce meaningful information. In the example above, key # 20 has a ranking ratio of 1.04 (the first place has a score of 0.0028 and the second 0.0027). Indeed, the perdicted twin key is not correct: the model did not rank the twin image at the first place.\n",
    "\n",
    "### Performance and computing time: parameter tuning\n",
    "\n",
    "#### Resoltion\n",
    "\n",
    "High resolution images usually yield better results, since they contain details that can be used for matching. However, higher resolution considerably increase computing time. When designing a model and a data collection protocol, it is worth playing around with a few data samples to find a good trade-off between the two. \n",
    "\n",
    "#### Blurring \n",
    "Up to a certain point, blurring can improve performance, as it denoises the images and prevents the algorithm from finding  useless keypoints. Consequently, it can also significantly reduces computing time. However, above a certain level, aggressive blurring leads to a loss of information and a weaker performance. As for the other parameters, the right trade-off depends on the nature of the images, and some experimenting will help you find the sweet spot that best suits your needs. \n",
    "\n",
    "By default, blurring is made with a sqare Gaussian kernel of the following side size:\n",
    "\n",
    "kernel_size = blurring_factor*(image_height + image_width)/2.\n",
    "\n",
    "The default blurring factor is set at 0.03. You can set it before reading the images in with:\n",
    "\n",
    "`DB.blurring_factor = value`\n",
    "\n",
    "#### Other parameters for advanced users\n",
    "\n",
    "Keypoint detections is implemented using the surf algorithm. You can set the [parameters of the algorithm](https://docs.opencv.org/3.4/d5/df7/classcv_1_1xfeatures2d_1_1SURF.html) to improve performance or reducing computing time. for instance:\n",
    "`DB.hessianThreshold = 500`, `DB.extended = 0` etc.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(imageSearch)",
   "language": "python",
   "name": "imagesearch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
